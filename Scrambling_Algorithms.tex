\section{Scrambler}
\label{sec:scrambling_algorithms}

	Due to radiation levels inside the detector chamber, the main data processing takes place in a concrete bunker away from the detector.
	To facilitate this, 20 optical links (per module) are used to transfer the data from the front end VELO to the DAQ FPGA.
	When communicating data digitally, the transferring module (TX) and the receiving module (RX) must have synchronised clocks.
	In this case, the GWT serialiser is the TX, and the DAQ is the RX.
	When achieving a synchronised clock, there are two main approaches:

	\begin{easylist}
		\ListProperties(Numbers=R,Margin=0.5cm,Align=fixed,FinalSpace=2em)
		% & Syncinize both the TX and RX from a single central clock - used in I$^2$C communication.
		& Transmit the TX clock with the data to the RX module - used in I$^2$C and SPI communication.
		& Use bit-changes in the data to continuously synchronised the RX clock.
	\end{easylist}

	The former of these options, although widely used in conventional electronics, requires a finely tuned clock accounting for all possible delays.
	The latter, while negating cons of the former, requires data with a high density of transitions to reduce the likelihood of a desynchronisation event.
	Because delays in the data are possible, the latter option has been selected.

	As mentioned, it is necessary to ensure that the data has a large density of transitions before being transmitted from the front-end detector to the DAQ module.
	However, as the majority of super pixel hit maps are empty, the data has a bias towards \textit{`0'}s.
	This reduces the frequency of transitions in the data - increasing the probability of a desynchronisation event.
	It is therefore necessary to scramble the data prior to transmission and descramble the data in the LLI of the DAQ FPGA.
	\par
	Scrambling and later descrambling the data is not a trivial exercise.
	The scrambling (TX) module and descrambling (RX) module must use a synchronised \textit{`key'}, that is used in both the scrambling and descrambling processes.
	In the FPGA, the \textit{`key'} is derived from the previous states of the data.
	There are two methods investigated for generating this \textit{`key'}:

	\begin{description}
		\item[Additive] The \textit{`key'} is generated by evolving the previous \textit{`key'} at each iteration of data using the incoming frame.
		\item[Multiplicative] The  \textit{`key'} is generated from the previous $n$ frames. (Here $n$ is a variable specific to the algorithm).
	\end{description}

	\subsection{Scrambler Options}
	\label{sub:scrambler_options}

		Three scrambling algorithms have been considered:

		\begin{description}
			\item[Additive Scrambler] \hfill \\
				This scrambler was originally implemented and used two sets of two-input XOR logic gates.
				As the name implies, this scrambler used additive key generation which is dependent on all previous input frames since the last reset signal.

			\item[Intermediate Scrambler] \hfill \\
				Created by Karol Hennessy, and deriving its name arbitrarily from the order of consideration, this multiplicative scrambler combines the current and previous frames to generate the \textit{`key'}.
				Therefore, in the event of desynchronisation, only two frames are lost before the \textit{`key'} is automatically recovered.
				This feature alone is a significant improvement over the Additive Scrambler.

			\item[VeloPix Scrambler] \hfill \\
				This is the current implemented scrambling algorithm in the DAQ and VeloPix code.
				Like the Intermediate Scrambler, it uses multiplicative \textit{`key'} generation.
				However, the VeloPix scrambler is compatible with further constraints enforced by the ASIC, including the number of combinational logic operations.
				The Intermediate Scrambler was design purely for simulation purposes and as such does not meet the additional ASIC constraints.
		\end{description}


	\subsection{Cross Checks} % (fold)
	\label{sub:cross_checks}
		The main priority when scrambling data, is ensuring that the data is recoverable.
		For all three scramblers, the algorithm was synthesised in Quartus \cite{ref:quartus} and simulated in Modelsim \cite{ref:modelsim}.
		The aim of synthesising and simulating the scramblers in these programs was to ensure that the design was both physical in terms of on-board logic gates, and to check that the scrambled data was recoverable, respectively.
		\par
		Furthermore, a C++ simulation was created for the three scramblers.
		This simulation had two main purposes;
		firstly to cross check the output of the C++ against the Modelsim simulations;
		secondly to simulate the scrambler over a much larger sample of data as Modelsim simulations are less time efficient.
		In addition to the cross checks, the C++ code allowed for the injection of a desychrnonisation event, in which the \textit{`key'} is lost.
		As expected, the Additive Scrambler was unable to recover any data post desychrnonisation, however the intermediate and VeloPix scramblers both recovered the \textit{`key'} after two frames and continued to decode data.

	% subsection cross_checks (end)

	\subsection{Algorithm Analysis}
	\label{sub:algorithm_analysis}

		For analytical purposes, it is assumed that fully scrambled data is indistinguishable from randomly generated data. 
		For this reason, the three algorithms are not only tested against each other and pre-scrambled simulated QWT data but also randomly generated binary.
		The randomly generated data was created using the Python \textit{`random'} library, selecting a \textit{`0'} or \textit{`1'} with equal probability.
		While the Python \textit{`random'} library is only pseudo-random, on the scale of this application (i.e. $>>$ 100,000 frames), it is sufficient for these purposes.
		\par
		A more mathematically rigorous approach, however, is to evaluate the system abstractly in the framework of statistical physics.
		In this abstraction, the 120 bit frame (with the header and parity removed)  is considered an ensemble; 
		microstates are the particular form of the frames;
		and macroscopic quantities can be calculated by averaging a large number of frames.
		For the analysis outlined in section \ref{subsub:messurements_of_the_algorithms}, predictions will be made using these principles and outlined in section \ref{subsub:statistical_predictions}.
		\par
		In the context of the statistical model, it is reasonable to consider the degree of \textit{`scrambled-ness'} analogous to entropy.	
		This analogy is not dissimilar to the common interpretation of entropy as a measure of disorder. 
		From Boltzmann's equation for entropy,

		\begin{equation}
			S \propto ln(\Omega)
			\label{eqn:boltzman}
		\end{equation}

		where $\Omega$ is the number of microstates associated with the macrostate, we learn that this state of maximum entropy is a macrostate with the maximum number of associated microstates.
		\par
		The entropic argument of Equation~\ref{eqn:boltzman} is not only mathematically founded. 
		For a scramble algorithm to hold for all possible data sets, it must also be capable of outputting all possible permutations. 
		As such, assuming all possible outputs are equally likely, the count of each macroscopic output will be proportional to the number of microstates associated.

		\subsubsection{Measurements of the Algorithms} 
		\label{subsub:messurements_of_the_algorithms}

			To compare the efficiency of the three algorithms in section \ref{sub:scrambler_options}, the algorithms where run over the same input data and compared for the following measures:

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					This measure counts the total number of bit transitions (i.e. $bit(n) \neq bit(n-1)$) in a 120 bit frame. 
					The header and parity information was not included as they are not scrambled.
					This is an important test as one of the roles of the scrambler is to maximise the number of transitions.

				\item[Common Bit Chain Length] \hfill \\
					One of the downfalls of the `Number of Transitions Per Frame' analysis is that the two hypothetical 20 bit frames,

					\begin{enumerate}[a)]
						\item \textsc{10101010101111111111},
						\item \textsc{10011001100110011001},
					\end{enumerate}

					both with 10 transitions, are considered to be equal. However, (b) is clearly a more suitable output for data transfer as (a) has a large probability of desynchronisation due to the long chains of \textit{`1'}s in the rightmost bits.
					It is therefore also necessary to evaluate the length of common bit chains within the scrambled data as shorter chains are more suitable for data transfer.

				\item[Bit Asymmetry] \hfill \\
					Pre-scramble, the data had a large bias towards \textit{`0'}s due to the majority of the hit maps being empty.
					Scrambled data, via entropic arguments, \textit{should} show zero bias either way.
					Therefore, by investigating how the number of \textit{`1'}s - \textit{`0'}s evolves over many frames, any bias in the scrambler can be found.

			\end{description}	

		\subsubsection{Statistical Predictions} % (fold)
		\label{subsub:statistical_predictions}

			\begin{description}
				\item[Number of Transitions Per Frame] \hfill \\
					
					The average number of transitions, $<N_\tau>$, is predicted to be,

					\begin{equation}
						<N_\tau> \ = \sum_{N_{\tau}=0}^{n-1} N_{\tau}\ f(N_{\tau}) = n\ p_{\tau} = 60,
						\label{eqn:tansition_expectation}
					\end{equation}

					with standard deviation,

					\begin{equation}
						\sigma_{N_\tau}^{Binomial} = \sqrt{ n\ p_{\tau}^2} = 5.48,
					\end{equation}

					where $p_\tau$ is the probability of the transition - i.e. any given bit being opposite to the previous bit.

				\item[Common Bit Chain Length] \hfill \\

					For a log graph of number of chains of length $n$, $log(N_n)$, against $n$ for a large sample of data, the gradient would be $log(1 - p_\tau)$.
					In this case, as $p_\tau = 0.5$, 

					\begin{equation}
						log(1 - p_\tau) = -0.30.
						\label{eqn:log_chain_length_gradient}			
					\end{equation}

				\item[Bit Asymmetry] \hfill \\

					The average asymmetry of 1's and 0's, $A_{1,0}$, is expected to be zero.

					The error on this value is calculated to be,

					\begin{equation}
						\sigma_{A_{1,0}} = \sqrt{<A_{1,0}^2> - <A_{1,0}>^2} = \sqrt{<A_{1,0}^2>} = \sqrt{n\ \Delta t},
					\end{equation}

					where $n \Delta t$ is the number of bits analysed. 

			\end{description}	

			Full derivations for the above quoted equations can be fount in Appendix~\ref{stat_der}

		\subsubsection{Results of Analysis}
		\label{subsub:algorithm_results}
			\begin{SCfigure}
				\centering
				\includegraphics[width=0.7\textwidth]{Transition_Histogram_update}
				\caption{Results of the \textit{`Number of Transitions Per Frame'} analysis. The results for the Random Data, Intermediate Scrambler and VeloPix Scrambler overlap.}
				\label{fig:transitions_per_frame}
			\end{SCfigure}
			\par
			\begin{SCfigure}
				\centering
				\includegraphics[width=0.7\textwidth]{Chain_length}
				\caption{Results of the \textit{`Common Bit Chain Length'} analysis. The results for the Random Data, Additive Scrambler, Intermediate Scrambler and VeloPix Scrambler overlap.}
				\label{fig:chain_length}
			\end{SCfigure}
			\par
			\begin{figure}[ht]
				\centering
				\includegraphics[width=1\textwidth]{Balance_graph_cropped}
				\caption{The results of the \textit{`Bit Asymetry'} analysis.}
				\label{fig:bit_asym}
			\end{figure}

			The results from the \textit{`Number of Transitions Per Frame'} analysis, shown in Figure~\ref{fig:transitions_per_frame}, show a strong similarity between the Intermediate and VeloPix Scramblers with the randomly generated data. 
			These results are within 1\% agreement with the theoretical predictions for $<N_\tau> = 60$ and $\sigma_{N_\tau} = 5.48$, made in Section~\ref{subsub:statistical_predictions}. 
			The remarkable consistency between the theoretical predictions and the randomly generated data provides confidence in both the theory, and the scrambled nature of the Intermediate and VeloPix scrambler outputs.
			\par
			For the \textit{`Common Bit Chain Length'} analysis all three scramblers; the random data, and the theoretical predictions are consistent to within 1\%. 
			Comparing the results for the Additive Scrambler, it is shown that while the frequency of longer chains is consistent with random data, the variance of transitions is larger than predicted. 
			Thus, the long and short trains are more locally clustered. 
			\par
			The \textit{`Bit Asymmetry'} of each scrambler, shown in Figure~\ref{fig:bit_asym}, is consistent with the theoretical prediction. 
			The deviation of $A_{1,0}$ for the predicted mean of 0 is fully consistent with stochastic noise. 
			The random data also shows consistency. 
			This gives confidence in the assumptions made in Section~\ref{subsub:statistical_predictions}.		
 			% \par		
 			% One notible feature of Figure\ref{fig:bit_asym} is the steap grandient of the additive scrambler a $t \sim 6.10^6$.		
 			% However, as the data stays within the theoretical limits and the \textit{`drop'} is of approximatly $\Delta A_{1,0} \sim 60.10^3$ over the range $n\ \Delta t \sim 1.2.10^8$ it would be difficult to construnct any argument claiming that this feature is of statistically significance.

	\subsection{Conclusion}

		\begin{table}[h]		
 			\centering		
 			\begin{TAB}(r)[7pt]{l|cc:cc}{c|c:ccc:cc}		
 							           & $<N_\tau>$ & $\sigma_{N_\tau}$ & Gradient  	& $p_\tau$    \\		
 				GQT data  		       & 54      	& 6.63           	& -0.268 		& 0.460 \\		
 				Additive Scrambler     & 60      	& 7.35           	& -0.305 		& 0.504 \\		
 				Intermediate Scrambler & 60      	& 5.45           	& -0.305 		& 0.504 \\		
 				Velopix Scrambler      & 60      	& 5.46           	& -0.305 		& 0.504 \\		
 				Random Data            & 60      	& 5.45           	& -0.305 		& 0.504 \\		
 				Theoretical Prediction & 60      	& 5.48           	& -0.3 			& 0.5   			
 			\end{TAB}		
 			\caption{The combined results of the algorithum analysis.}		
 			\label{tab:comb_results}		
 		\end{table}		
 		
 		The consistency of random data and the theoretical predictions justifies the assumptions and approximations made in Section~\ref{sub:algorithm_analysis} and Section~\ref{subsub:statistical_predictions}. 
 		Furthermore, the confirmation of the statistical model allows for accurate comparisons to be made from predicted values and their measured counterparts.		
 		\par		
 		The Additive Scrambler, while consistent with the \textit{`Chain Length'} and \textit{`Bit Asymmetry'} analysis, has a variance in the transition frequency that leads to the conclusion that long and short chains are locally clustered. 		
 		This is not ideal for data transfer. 		
 		Many sequential long chains increase the probability of TX-RX clock desynchronisation. 		
 		Furthermore, the additive scrambler will not recover from this loss of synchronisation, as the \textit{`key'} will never be recovered without a common reset signal.		
 		\par		
 		The Intermediate Scrambler produced an output consistent with random data. 		
 		This makes the algorithm suitable for data transfer.		
 		As already mentioned, however, the scrambler is designed for computer simulation.		
 		As such, it is not suitable for implementation as it does not meet the addition requirements of the ASIC.		
 		\par		
 		The VeloPix Scrambler, like the Intermediate Scrambler, produces a statistically scrambled output.		
 		Furthermore, the algorithm is in line with the additional requirements of the ASIC.		
 		As such, it is ideal for implementation, and hence is currently the choice algorithm for use in the 2019 VELO upgrade.







