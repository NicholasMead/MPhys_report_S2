	
	\begin{description}
		\item[Number of Transitions Per Frame] \hfill \\
			
			Consider a particle in a symmetric, discrete time-dependent, two state system,

			\begin{equation}
				p_0(t) = p_1(t) = 0.5 \quad : \quad \forall\ t \in \mathbb{N},
			\end{equation}

			At each time iteration,

			\begin{equation}
				p_{i \to j}(t) = 0.5 \quad : \quad i,j = [0\ 1], \quad \forall\ t \in \mathbb{N}.
			\end{equation}

			However, assuming zero bias and detailed balance, as $p_{1 \to 0}(t)$ is equal in both probability and importance to $p_{0 \to 1}(t)$, the probability of a bit change shall henceforth be refereed to as $p_{\tau}(t)$.
			\par
			Over an $n$ step process, analogous to an $n$ bit frame, the probability distribution of the number of transitions $N_\tau$ is given by Binomial statistics,

			\begin{equation}
				f(N_{\tau}) = \frac{n!}{N_{\tau}!(n-N_{\tau})!}\ p^{N_{\tau}}\ (1 - p)^{n-N_{\tau}}
			\end{equation}

			Simplified for the special case $p = p_{\tau} = 0.5$,

			\begin{equation}
				f_{\tau}(N_{\tau}) = \frac{n!}{N_{\tau}!(n-N_{\tau})!}\ (p_{\tau})^{n}
				\label{eqn:transition_propability_dencity}
			\end{equation}

			For $n = 120$, we can calculate,

			\begin{equation}
				<N_\tau>^{Binomial} \ = \sum_{N_{\tau}=0}^{n-1} N_{\tau}\ f(N_{\tau}) = n\ p_{\tau} = 60
				\label{eqn:tansition_expectation}
			\end{equation}

			\begin{equation}
				\sigma_{N_\tau}^{Binomial} = \sqrt{ n\ p_{\tau}^2} = 5.48
			\end{equation}

			Furthermore, when considering the entropic argument of equation \ref{eqn:boltzman}, the number of microstates corresponding to each macrostate $N_\tau$ can be related to equation \ref{eqn:transition_propability_dencity},

			\begin{equation}
				\Omega_\tau \sim \frac{n!}{N_{\tau}!(n-N_{\tau})!}
			\end{equation}

			\begin{equation}
				<N_\tau>^{Entropic} = MAX[S_\tau] = MAX[\Omega_\tau]
			\end{equation}

			This can be numerically solved,

			\begin{equation}
				<N_\tau>^{Entropic}\ = 60
				\label{eqn:n_t_entropic}
			\end{equation}

			While the result of equation \ref{eqn:n_t_entropic} does not contibute anything new, it is important as a \textit{`sanity check'}.
			Because the system can be described as in section \ref{sub:algorithm_analysis}, it would indicate a problem in the theoretical framework if the result did not match.


		\item[Common Bit Chain Length] \hfill \\
			
			The probability of a chain of length $n$ is,

			\begin{equation}
				p_n = p_1(1 - p_\tau)^{n-1}, \quad : \quad n \in \mathbb{N}, \quad n > 1
			\end{equation}

			where $p_1$ is the number of chains of length 1. 
			As $p_1 = N_0 (1 -p_\tau)$, where $N_0$ is the total number of chains,

			\begin{equation}
				\frac{N_n}{N_0} = (1 - p_\tau)^n, \quad : \quad n \in \mathbb{N}, \quad n > 1
			\end{equation}

			where $N_n$ is the number of chains of length $n$.
			Taking the logarithm of both sides,

			\begin{align}
				log\left(\frac{N_n}{N_0}\right) &= n\ log(1 - p_\tau), \nonumber \\
					log(N_n) &= n\  log(1 - p_\tau) + log(N_0).
				\end{align}

			Therefore, for a graph of $log(N_n)$ against $n$ for a large sample of data, the gradient would be $log(1 - p_\tau)$.
			In this case, as $p_\tau = 0.5$, 

			\begin{equation}
				log(1 - p_\tau) = -0.30\ .
				\label{eqn:log_chain_length_gradient}			
			\end{equation}

		\item[Bit Asymmetry] \hfill \\
			
			$A_{1,0}$, the asymmetry of \textit{`1'}s and \textit{`0'}s is defined as,

			\begin{equation}
				A_{1,0} = N_1 - N_0,
				\label{eqn:a_def}
			\end{equation}

			where $N_1$ and $N_0$ are the number of \textit{`1'}s and \textit{`0'}s respectively.
			We can consider the evolution of $A_{1,0}$ with frame $t$ of size $n$ as a stochastic iterative map with zero deterministic growth \cite{ref:stockastic_physics},

			\begin{equation}
				A_{1,0}(nt + n\ \Delta t) = A_{1,0}(nt) + \mathcal{N}(nt).
			\end{equation}

			Where $\mathcal{N}$ is an independant random variable picked from a gaussian distribution. While $A_{1,0}(t) \in \mathbb{Z}$, in the limit of large $nt$ we can approximate that $A_{1,0}$ is continous. 
			\par
			If we consider the moments of $A_{1,0}$,

			\begin{align}
				\label{eqn:A_moment1}
				<A_{1,0}(nt = M\ n\ \Delta t)> & = \sum_{m = 0}^{M -1}  \mathcal{N}(m\ n\ \Delta t), \\
				\label{eqn:A_moment2}
				<A_{1,0}(nt = M\ n\ \Delta t)^2> & = \sum_{m=0}^{M-1} \sum_{m'=0}^{M-1}  \mathcal{N}(m\ n\ \Delta t) \mathcal{N}(m'\ n\ \Delta t)\ \delta_{mm'} \nonumber \\
				&= \sum_{m=0}^{M-1} < \mathcal{N}(m\ n\ \Delta t)^2 >.
			\end{align}

			Clearly, in Equation \ref{eqn:A_moment1}, $<A_{1,0}> = 0$. In Equation \ref{eqn:A_moment2}, we assume the variance is of form $(n\ \Delta t)^\alpha$ \cite{ref:stockastic_physics}. Then,

			\begin{equation}
				<A_{1,0}(nt = M\ n\ \Delta t)^2>\ = M (n\ \Delta t)^\alpha.
				\label{eqn:A_moment3}
			\end{equation}

			Running the analysis over the frames $t = 0$ to $t_f$, the number of bits sampled is $M = {t_f / n\ \Delta t}$. Substituting this into Equation \ref{eqn:A_moment3},

			\begin{equation}
				<A_{1,0}(nt = M\ n\ \Delta t)^2>\ = t_f\ (n\ \Delta t)^{\alpha -1}.
			\end{equation}

			Considering the three cases of $\alpha$ in the approximation of continous $n \Delta t$:
			
			\vspace{1em}

			\begin{easylist}[itemize]
				% \ListProperties{Margin=2cm,Align=fixed,FinalSpace=2em}
				& $\bm{\alpha > 1}$: Here $A_{1,0} \to 0$ as $\Delta t \to 0$.
				& $\bm{\alpha < 1}$: Here $A_{1,0} \to \infty$ as $\Delta t \to 0$.
				& $\bm{\alpha = 1}$: This is the only sensible choice.
			\end{easylist}
			
			\vspace{1em}

			With $\alpha =1$,

			\begin{equation}
				<A_{1,0}(nt = M\ n\ \Delta t)^2> = M (n\ \Delta t).
			\end{equation}

			And thus,

			\begin{equation}
				\sigma_{A_{1,0}} = \sqrt{<A_{1,0}^2> - <A_{1,0}>^2} = \sqrt{<A_{1,0}^2>} = \sqrt{n\ \Delta t}.
			\end{equation}

	\end{description}	